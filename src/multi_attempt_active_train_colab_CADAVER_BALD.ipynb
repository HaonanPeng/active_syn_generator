{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":565,"status":"ok","timestamp":1621275008077,"user":{"displayName":"Haonan Peng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4GmArzFW7K2Q2E1X7f2__FMNgtwjOtMJYWb1k=s64","userId":"08169864112056329318"},"user_tz":420},"id":"MwAnnaX7eVvz","outputId":"e4578917-4a23-41b2-fddd-f6bb709e9d82"},"outputs":[],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKVmF0APRk7-"},"outputs":[],"source":["import sys\n","path_workspace = '/content/active_learning_v1'\n","path_source = '/content/drive/MyDrive/active_learning_ws/active_learning_v1'\n","sys.path.append(path_source)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88403,"status":"ok","timestamp":1621275095927,"user":{"displayName":"Haonan Peng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4GmArzFW7K2Q2E1X7f2__FMNgtwjOtMJYWb1k=s64","userId":"08169864112056329318"},"user_tz":420},"id":"RoOYVvPJUarY","outputId":"11ab399c-96cf-4037-f52e-38e18459319f"},"outputs":[],"source":["# !pip uninstall tensorflow\n","!pip install tensorflow==1.13.1\n","!pip install tensorflow-gpu==1.13.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5BWVDa9Fk5z"},"outputs":[],"source":["import shutil\n","!mkdir '/content/active_learning_v1'\n","\n","shutil.copy(\"/content/drive/MyDrive/active_learning_ws/active_learning_v1/labels.json\", \n","            \"/content/active_learning_v1/labels.json\")\n","\n","!unzip -q \"/content/drive/MyDrive/active_learning_ws/data/database_folder12_c.zip\" -d '/content/active_learning_v1'\n","!unzip -q \"/content/drive/MyDrive/active_learning_ws/data/one_img_background_for_syn_folder12_c.zip\" -d '/content/active_learning_v1'\n","!unzip -q \"/content/drive/MyDrive/active_learning_ws/active_learning_v1/mobilenet_v1_1.0_224.zip\" -d '/content/active_learning_v1'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102835,"status":"ok","timestamp":1621275110369,"user":{"displayName":"Haonan Peng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4GmArzFW7K2Q2E1X7f2__FMNgtwjOtMJYWb1k=s64","userId":"08169864112056329318"},"user_tz":420},"id":"WDDRDTWPRvSJ","outputId":"e6c75bd6-871e-429b-c193-78bdb504e283"},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","#from model import *\n","from model_mobilenet_no_mafa import *\n","from utils import *\n","import tensorflow as tf\n","import time\n","import shutil\n","import evaluation_contour\n","\n","\n","from PIL import Image\n","import active_learner as al\n","import active_syn_generator as asg\n","from generator_utils import rand_from_range\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50073746,"status":"ok","timestamp":1621325081286,"user":{"displayName":"Haonan Peng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4GmArzFW7K2Q2E1X7f2__FMNgtwjOtMJYWb1k=s64","userId":"08169864112056329318"},"user_tz":420},"id":"ofM3XO9YUz_z","outputId":"e7bd149f-271a-4718-b8c9-7772d08cd599"},"outputs":[],"source":["active_iterations = 3\n","\n","proportion_labeled = 0.10\n","proportion_test = 0.0\n","\n","head_query_per_iter = 0.05\n","query_skip = 2\n","random_query_per_iter = 0.000\n","query_per_iter = head_query_per_iter + random_query_per_iter\n","\n","syn1_per_query = 2\n","syn2_per_query = 0\n","multi_gen = 1\n","\n","border_fusion = True\n","extract_dilation = 15\n","fusion_blur = (5, 10)\n","\n","min_epoch = 20\n","max_epoch = 5000\n","epoch = 20 + int(10/(proportion_labeled + (active_iterations-1)*query_per_iter))\n","epoch = np.max((min_epoch, epoch))\n","epoch = np.min((max_epoch, epoch))\n","\n","init_bkgd = True\n","generate_bkgd = True\n","\n","img_size = (240,240)\n","batch_size = 16\n","check_point_folder_name = '/DeepLab_' + str(batch_size) + '_' + str(img_size[1]) + '_' + str(img_size[0])\n","\n","final_vali_rst = []\n","final_test_dice = []\n","final_test_iou = []\n","final_test_iou_nb = []\n","final_test_iou_rs = []\n","\n","for attempt in range(1, 6):\n","  folder_workspace = None\n","  active_learner1 = None\n","  active_syn_generator1 = None\n","  if attempt == 1:\n","    al.create_folder(path_workspace)\n","    folder_workspace = path_workspace + '/active_learning_try1'\n","    if os.path.isdir(folder_workspace):\n","        print('workspace already exist, please change a new one')\n","        sys.exit()\n","    al.create_folder(folder_workspace)\n","\n","    %cp -ar '/content/active_learning_v1/raw_data_base' '/content/active_learning_v1/active_learning_try1'\n","    %cp -ar '/content/active_learning_v1/test_set' '/content/active_learning_v1/active_learning_try1'\n","    %cp -ar '/content/active_learning_v1/init_background' '/content/active_learning_v1/active_learning_try1'\n","\n","    random_seed = 13\n","\n","  if attempt == 2:\n","    al.create_folder(path_workspace)\n","    folder_workspace = path_workspace + '/active_learning_try2'\n","    if os.path.isdir(folder_workspace):\n","        print('workspace already exist, please change a new one')\n","        sys.exit()\n","    al.create_folder(folder_workspace)\n","\n","    %cp -ar '/content/active_learning_v1/raw_data_base' '/content/active_learning_v1/active_learning_try2'\n","    %cp -ar '/content/active_learning_v1/test_set' '/content/active_learning_v1/active_learning_try2'\n","    %cp -ar '/content/active_learning_v1/init_background' '/content/active_learning_v1/active_learning_try2'\n","\n","    random_seed = 23\n","\n","  if attempt == 3:\n","    al.create_folder(path_workspace)\n","    folder_workspace = path_workspace + '/active_learning_try3'\n","    if os.path.isdir(folder_workspace):\n","        print('workspace already exist, please change a new one')\n","        sys.exit()\n","    al.create_folder(folder_workspace)\n","\n","    %cp -ar '/content/active_learning_v1/raw_data_base' '/content/active_learning_v1/active_learning_try3'\n","    %cp -ar '/content/active_learning_v1/test_set' '/content/active_learning_v1/active_learning_try3'\n","    %cp -ar '/content/active_learning_v1/init_background' '/content/active_learning_v1/active_learning_try3'\n","\n","    random_seed = 33\n","\n","  if attempt == 4:\n","    al.create_folder(path_workspace)\n","    folder_workspace = path_workspace + '/active_learning_try4'\n","    if os.path.isdir(folder_workspace):\n","        print('workspace already exist, please change a new one')\n","        sys.exit()\n","    al.create_folder(folder_workspace)\n","\n","    %cp -ar '/content/active_learning_v1/raw_data_base' '/content/active_learning_v1/active_learning_try4'\n","    %cp -ar '/content/active_learning_v1/test_set' '/content/active_learning_v1/active_learning_try4'\n","    %cp -ar '/content/active_learning_v1/init_background' '/content/active_learning_v1/active_learning_try4'\n","\n","    random_seed = 43\n","\n","  if attempt == 5:\n","    al.create_folder(path_workspace)\n","    folder_workspace = path_workspace + '/active_learning_try5'\n","    if os.path.isdir(folder_workspace):\n","        print('workspace already exist, please change a new one')\n","        sys.exit()\n","    al.create_folder(folder_workspace)\n","\n","    %cp -ar '/content/active_learning_v1/raw_data_base' '/content/active_learning_v1/active_learning_try5'\n","    %cp -ar '/content/active_learning_v1/test_set' '/content/active_learning_v1/active_learning_try5'\n","    %cp -ar '/content/active_learning_v1/init_background' '/content/active_learning_v1/active_learning_try5'\n","\n","    random_seed = 53\n","\n","\n","\n","  show_img = False # for debug, not working for ubuntu\n","\n","\n","  tf.set_random_seed(random_seed)\n","  random.seed(random_seed)\n","  np.random.seed(random_seed)\n","\n","  start_time = time.time()\n","\n","  active_learner1 = al.active_learner()\n","  active_learner1.folder_workspace = folder_workspace\n","  active_learner1.init_active_learner(proportion_test = proportion_test, \n","                                      proportion_labeled = proportion_labeled, \n","                                      query_criterion = 'BALD')\n","\n","  active_syn_generator1 = asg.active_syn_generator()\n","  active_syn_generator1.init_syn_img_generator(folder_learn_iter = active_learner1.folder_learn_iter[0],\n","                                              extract_dilation = extract_dilation, \n","                                              img_size = img_size,\n","                                              init_bkgd = init_bkgd, \n","                                              generate_bkgd = generate_bkgd,\n","                                              output_process = False)\n","\n","  vali_evaluation_result = []\n","  test_dice = []\n","  test_iou = []\n","  test_iou_nb = []\n","  test_iou_rs = []\n","\n","  for a_iter in range(0, active_iterations):\n","      active_learner1.cur_iter = a_iter\n","      if a_iter==0:\n","          active_syn_generator1.load_tool_and_bkgd()\n","          active_syn_generator1.generate_background()\n","          #active_syn_generator1.load_tool_and_bkgd()\n","          query_img_list_ = al.get_file_names(active_learner1.folder_query_added[a_iter] + '/images', 'jpg')\n","          \n","          for query_img_file in query_img_list_:\n","              query_label_file = query_img_file[:-3] + 'png'\n","              \n","              image = cv2.imread(active_learner1.folder_query_added[a_iter] + '/images/' + query_img_file)\n","              label = cv2.imread(active_learner1.folder_query_added[a_iter] + '/labels/' + query_label_file, cv2.IMREAD_GRAYSCALE)\n","              \n","              if np.sum(label) < 40: # if the query image has no tool or only a small tool\n","                  continue\n","              active_syn_generator1.file_name = query_img_file[:-4]\n","              \n","              syn1_images_, syn1_labels_, syn2_images_, syn2_labels_ = active_syn_generator1.generate_syn_img(image, label, dilation = extract_dilation,\n","                                                                                                              syn1 = syn1_per_query, syn2 = syn2_per_query, \n","                                                                                                              multi_gen = multi_gen,                                                                                           \n","                                                                                  flip=(0.1, 0.8), \n","                                                                                  shrink_factor=(0.9, 1.2), \n","                                                                                  x_factor=(-0.1, 0.1), \n","                                                                                  y_factor=(-0.1, 0.1), \n","                                                                                  r_factor=(-30, 30), \n","                                                                                  color_adjust_strength=(0.4, 1.0), \n","                                                                                  brightness_adjust=(0.9, 1.3), \n","                                                                                  border_fusion = border_fusion,\n","                                                                                  fusion_blur= fusion_blur, \n","                                                                                  border_center=((115,125), (115,125)), \n","                                                                                  radius=(150,170), \n","                                                                                  ksize=(1,3), \n","                                                                                  sig_x=(-10,3),\n","                                                                                  elastic= (-2, -1), \n","                                                                                  els_alpha=(2000, 3000), \n","                                                                                  els_sigma=(10, 15), \n","                                                                                  dila_ero=(-0.5, 0.5), \n","                                                                                  val_type=0, \n","                                                                                  dila_ero_size=(1, 2), \n","                                                                                  img_elastic=(-4, -1), \n","                                                                                  img_els_alpha=(2000, 3000), \n","                                                                                  img_els_sigma=(10, 15),\n","                                                                                  img_size = img_size,\n","                                                                                  show_img = show_img)\n","              al.create_folder(active_learner1.folder_query_added[a_iter] + '/img_n_gt')\n","              count = 0\n","              for syn1_image in syn1_images_:\n","                  syn1_label = syn1_labels_[count]\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/images/' + query_img_file[:-4] + '_syn1_' + str(count) + '.jpg', syn1_image)\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/labels/' + query_img_file[:-4] + '_syn1_' + str(count) + '.png', syn1_label)\n","                  count = count + 1\n","                  \n","                  ground_truth = np.zeros(syn1_image.shape)\n","                  ground_truth[:,:,1] = (syn1_label * 250).clip(0,255).astype(np.uint8)\n","                  img_n_gt = np.hstack((syn1_image, ground_truth))\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/img_n_gt/' + query_img_file[:-4] + '_syn1_' + str(count) + '.jpg', img_n_gt)\n","                  \n","              count = 0\n","              for syn2_image in syn2_images_:\n","                  syn2_label = syn2_labels_[count]\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/images/' + query_img_file[:-4] + '_syn2_' + str(count) + '.jpg', syn2_image)\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/labels/' + query_img_file[:-4] + '_syn2_' + str(count) + '.png', syn2_label)\n","                  count = count + 1\n","                  \n","                  ground_truth = np.zeros(syn2_image.shape)\n","                  ground_truth[:,:,1] = (syn2_label * 250).clip(0,255).astype(np.uint8)\n","                  img_n_gt = np.hstack((syn2_image, ground_truth))\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/img_n_gt/' + query_img_file[:-4] + '_syn2_' + str(count) + '.jpg', img_n_gt)\n","          al.copy_folder2(active_learner1.folder_query_added[a_iter], active_learner1.folder_labeled[a_iter])  \n","\n","      if a_iter>0:\n","          active_learner1.create_folders(iteration = a_iter)\n","          active_learner1.inherit_from_previous_iter()\n","          \n","          active_syn_generator1.init_new_iteration(iteration = a_iter, \n","                                                  folder_learn_iter = active_learner1.folder_learn_iter[a_iter], \n","                                                  dilation = extract_dilation)\n","          \n","          active_syn_generator1.load_tool_and_bkgd()\n","          \n","          query_img_list_ = al.get_file_names(active_learner1.folder_query_added[a_iter] + '/images', 'jpg')\n","          \n","          for query_img_file in query_img_list_:\n","              query_label_file = query_img_file[:-3] + 'png'\n","              \n","              image = cv2.imread(active_learner1.folder_query_added[a_iter] + '/images/' + query_img_file)\n","              label = cv2.imread(active_learner1.folder_query_added[a_iter] + '/labels/' + query_label_file, cv2.IMREAD_GRAYSCALE)\n","              \n","              if np.sum(label) < 40: # if the query image has no tool or only \n","                  continue\n","          \n","              syn1_images_, syn1_labels_, syn2_images_, syn2_labels_ = active_syn_generator1.generate_syn_img(image, label, dilation = extract_dilation,\n","                                                                                                              syn1 = syn1_per_query, syn2 = syn2_per_query, \n","                                                                                                              multi_gen = multi_gen,                                                                                           \n","                                                                                  flip=(0.1, 0.8), \n","                                                                                  shrink_factor=(0.9, 1.2), \n","                                                                                  x_factor=(-0.1, 0.1), \n","                                                                                  y_factor=(-0.1, 0.1), \n","                                                                                  r_factor=(-30, 30), \n","                                                                                  color_adjust_strength=(0.4, 1.0), \n","                                                                                  brightness_adjust=(0.9, 1.3),\n","                                                                                  border_fusion = border_fusion, \n","                                                                                  fusion_blur= fusion_blur, \n","                                                                                  border_center=((115,125), (115,125)), \n","                                                                                  radius=(150,170), \n","                                                                                  ksize=(1,3), \n","                                                                                  sig_x=(-10,3),\n","                                                                                  elastic= (-2, -1), \n","                                                                                  els_alpha=(2000, 3000), \n","                                                                                  els_sigma=(10, 15), \n","                                                                                  dila_ero=(-0.5, 0.5), \n","                                                                                  val_type=0, \n","                                                                                  dila_ero_size=(1, 2), \n","                                                                                  img_elastic=(-4, -1), \n","                                                                                  img_els_alpha=(2000, 3000), \n","                                                                                  img_els_sigma=(10, 15),\n","                                                                                  img_size = img_size,\n","                                                                                  show_img = show_img)\n","              al.create_folder(active_learner1.folder_query_added[a_iter] + '/img_n_gt')\n","              count = 0\n","              for syn1_image in syn1_images_:\n","                  syn1_label = syn1_labels_[count]\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/images/' + query_img_file[:-4] + '_syn1_' + str(count) + '.jpg', syn1_image)\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/labels/' + query_img_file[:-4] + '_syn1_' + str(count) + '.png', syn1_label)\n","                  count = count + 1\n","                  \n","                  ground_truth = np.zeros(syn1_image.shape)\n","                  ground_truth[:,:,1] = (syn1_label * 250).clip(0,255).astype(np.uint8)\n","                  img_n_gt = np.hstack((syn1_image, ground_truth))\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/img_n_gt/' + query_img_file[:-4] + '_syn1_' + str(count) + '.jpg', img_n_gt)\n","                  \n","              count = 0\n","              for syn2_image in syn2_images_:\n","                  syn2_label = syn2_labels_[count]\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/images/' + query_img_file[:-4] + '_syn2_' + str(count) + '.jpg', syn2_image)\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/labels/' + query_img_file[:-4] + '_syn2_' + str(count) + '.png', syn2_label)\n","                  count = count + 1\n","                  \n","                  ground_truth = np.zeros(syn2_image.shape)\n","                  ground_truth[:,:,1] = (syn2_label * 250).clip(0,255).astype(np.uint8)\n","                  img_n_gt = np.hstack((syn2_image, ground_truth))\n","                  cv2.imwrite(active_learner1.folder_query_added[a_iter] + '/img_n_gt/' + query_img_file[:-4] + '_syn2_' + str(count) + '.jpg', img_n_gt)\n","          al.copy_folder2(active_learner1.folder_query_added[a_iter], active_learner1.folder_labeled[a_iter])\n","                            \n","  # Training CNN -----------------------------------------------------------------------------------\n","      def del_all_flags(FLAGS):\n","          flags_dict = FLAGS._flags()\n","          keys_list = [keys for keys in flags_dict]\n","          for keys in keys_list:\n","              FLAGS.__delattr__(keys)\n","      del_all_flags(tf.flags.FLAGS)\n","      tf.reset_default_graph()\n","      tf.set_random_seed(random_seed)\n","      \n","      flags = tf.app.flags\n","      tf.app.flags.DEFINE_string('f', '', 'kernel')\n","      print('||||||||||||||||||||||||||||||||||')\n","      print('current epoch:')\n","      print(str(epoch))\n","      print('||||||||||||||||||||||||||||||||||')\n","      \n","      flags.DEFINE_integer(\"epoch\", int(epoch), \"Epoch to train [25]\")\n","      \n","      # flags.DEFINE_integer(\"epoch\",epoch, \"Epoch to train [25]\")\n","      flags.DEFINE_integer(\"batch_size\", batch_size, \"The size of batch images [64]\")\n","      flags.DEFINE_integer(\"input_height\", img_size[1], \"The size of image to use (will be center cropped). [108]\")\n","      flags.DEFINE_integer(\"input_width\", img_size[0], \"The size of image to use (will be center cropped). If None, same value as input_height [None]\")\n","      \n","      # the folder of training set and validation set, must contain a sub-folder called 'images' and a sub-folder called 'labels' \n","      flags.DEFINE_string(\"train_dataset\", active_learner1.folder_labeled[a_iter], \"train dataset direction\")\n","      flags.DEFINE_string(\"val_dataset\", active_learner1.folder_unlabeled[a_iter], \"train dataset direction\")\n","      \n","      flags.DEFINE_string(\"img_pattern\", \"*.jpg\", \"Glob pattern of filename of input images [*]\")\n","      flags.DEFINE_string(\"label_pattern\", \"*.png\", \"Glob pattern of filename of input labels [*]\")\n","      flags.DEFINE_string(\"checkpoint_dir\", active_learner1.folder_checkpoint[a_iter], \"Directory name to save the checkpoints [checkpoint]\")\n","      flags.DEFINE_string(\"pretrain_dir\", path_workspace + \"/mobilenet_v1_1.0_224\", \"\")\n","      flags.DEFINE_string(\"gpu\", '0', \"gpu\")\n","      FLAGS = flags.FLAGS\n","      \n","      pp.pprint(flags.FLAGS.__flags)\n","      \n","      print(active_learner1.folder_checkpoint[a_iter])\n","      if not os.path.exists(FLAGS.checkpoint_dir):\n","        os.makedirs(FLAGS.checkpoint_dir)\n","      #al.create_folder(active_learner1.folder_checkpoint[a_iter])\n","        \n","      color_table = load_color_table(path_workspace + '/labels.json')\n","      #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n","      run_config = tf.ConfigProto()\n","      run_config.gpu_options.allow_growth=True\n","      tf.reset_default_graph()\n","      with tf.Session(config=run_config) as sess:\n","        \n","        net = DeepLab(\n","              sess,\n","              input_width=FLAGS.input_width,\n","              input_height=FLAGS.input_height,\n","              batch_size=FLAGS.batch_size,\n","              img_pattern=FLAGS.img_pattern,\n","              label_pattern=FLAGS.label_pattern,\n","              checkpoint_dir=FLAGS.checkpoint_dir,\n","              pretrain_dir=FLAGS.pretrain_dir,\n","              train_dataset=FLAGS.train_dataset,\n","              val_dataset=FLAGS.val_dataset,\n","              num_class=2,\n","              color_table=color_table,is_train=True)\n","        \n","        net.train(FLAGS)\n","          \n","  # Evaluating CNN and query labeling using validation (unlabeled data)-----------------------------------------------------------------------\n","      def del_all_flags(FLAGS):\n","          flags_dict = FLAGS._flags()\n","          keys_list = [keys for keys in flags_dict]\n","          for keys in keys_list:\n","              FLAGS.__delattr__(keys)\n","      del_all_flags(tf.flags.FLAGS)\n","      tf.reset_default_graph()\n","      \n","      flags = tf.app.flags\n","      tf.app.flags.DEFINE_string('f', '', 'kernel')\n","      flags.DEFINE_string(\"train_dataset\", active_learner1.folder_labeled[a_iter], \"train dataset direction\")\n","      flags.DEFINE_string(\"val_dataset\", active_learner1.folder_unlabeled[a_iter], \"val dataset direction\")\n","      flags.DEFINE_string(\"checkpoint_dir\", active_learner1.folder_checkpoint[a_iter] + check_point_folder_name, \"checkpoint\")\n","      flags.DEFINE_string(\"img_dir\", active_learner1.folder_unlabeled[a_iter] + '/images', \"img_dir\")\n","      flags.DEFINE_string(\"rst_dir\", active_learner1.folder_learn_iter[a_iter] + \"/vali-rsts\", \"rst_dir\")\n","      flags.DEFINE_string(\"gt_dir\", active_learner1.folder_unlabeled[a_iter] + '/labels', \"gt_dir\")\n","      flags.DEFINE_string(\"rst_file\", active_learner1.folder_learn_iter[a_iter] + \"_vali_rst.txt\", \"gt_dir\")\n","      flags.DEFINE_string(\"gpu\", '0', \"gpu\")\n","      FLAGS = flags.FLAGS\n","      \n","      os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu\n","      \n","      test_all=True\n","      color_table = load_color_table(path_workspace + '/labels.json')\n","      run_config = tf.ConfigProto()\n","      sess=tf.Session(config=run_config)\n","      with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n","          net = DeepLab(\n","                sess,\n","                input_width=img_size[0],\n","                input_height=img_size[1],\n","                batch_size=1,\n","                img_pattern=\"*.jpg\",\n","                label_pattern=\"*.png\",\n","                checkpoint_dir=FLAGS.checkpoint_dir,\n","                pretrain_dir='',\n","                train_dataset=FLAGS.train_dataset,\n","                val_dataset=FLAGS.val_dataset,\n","                num_class=2,\n","                color_table=color_table,is_train=False)\n","          if not net.load(net.checkpoint_dir)[0]:\n","              raise Exception(\"Cannot find checkpoint!\")\n","              \n","          if test_all:\n","          \n","              #test on train\n","              img_dir = FLAGS.img_dir\n","              rst_dir = FLAGS.rst_dir\n","              gt_dir = FLAGS.gt_dir\n","              \n","              if not os.path.exists(rst_dir):\n","                  os.makedirs(rst_dir)\n","      \n","              \n","              files=os.listdir(img_dir)\n","              for i,file in enumerate(files):\n","                  if not file.endswith(\".jpg\"):\n","                      continue\n","                  \n","                  \n","                  img = cv2.imread(os.path.join(img_dir,file))\n","\n","                  # Monte Carlo query method here\n","                  MCDP_times = 10\n","\n","                  mean_out_put_pd = np.zeros((img_size[1], img_size[0], 2))\n","                  BALD_img_pd_ = []\n","                  for MC_iter in range(0, MCDP_times):\n","                    tf.set_random_seed(11*MC_iter + random_seed)\n","                    random.seed(11*MC_iter + random_seed)\n","                    np.random.seed(11*MC_iter + random_seed)\n","                    idxmap, colormap, out_put_pd = net.inference(img, drop_out = 0.5)  \n","                    BALD_img_pd_.append(out_put_pd)\n","                    mean_out_put_pd = mean_out_put_pd + (1/MCDP_times) * out_put_pd\n","                  \n","                  tf.set_random_seed(random_seed)\n","                  random.seed(random_seed)\n","                  np.random.seed(random_seed)\n","                  active_learner1.add_instance(img_pd = mean_out_put_pd, file_name = file, BALD_img_pd_ = BALD_img_pd_)\n","                  # al.show_prob_distribution('image_porb', out_put_pd)\n","                  \n","                  colormap=cv2.cvtColor(colormap,cv2.COLOR_RGB2BGR)\n","                  \n","                  cv2.imwrite(os.path.join(rst_dir,file[:-4]+'.png'),colormap) \n","                  \n","              mean_dice, mean_iou = evaluate_seg_result(rst_dir, gt_dir, FLAGS.rst_file)\n","              \n","              vali_evaluation_result.append([mean_dice, mean_iou])\n","              \n","              active_learner1.sort_query_list()\n","              active_learner1.show_query_list(head = 10)\n","              active_learner1.select_query(head_query = head_query_per_iter, skip = query_skip, random_query = random_query_per_iter)\n","              \n","  # Evaluating CNN and using test set-----------------------------------------------------------------------\n","      def del_all_flags(FLAGS):\n","          flags_dict = FLAGS._flags()\n","          keys_list = [keys for keys in flags_dict]\n","          for keys in keys_list:\n","              FLAGS.__delattr__(keys)\n","      del_all_flags(tf.flags.FLAGS)\n","      tf.reset_default_graph()\n","      \n","      \n","      flags = tf.app.flags\n","      tf.app.flags.DEFINE_string('f', '', 'kernel')\n","      flags.DEFINE_string(\"train_dataset\", active_learner1.folder_labeled[a_iter], \"train dataset direction\")\n","      flags.DEFINE_string(\"val_dataset\", active_learner1.folder_unlabeled[a_iter], \"val dataset direction\")\n","      flags.DEFINE_string(\"checkpoint_dir\", active_learner1.folder_checkpoint[a_iter] + check_point_folder_name, \"checkpoint\")\n","      flags.DEFINE_string(\"img_dir\", active_learner1.folder_test_set + '/images', \"img_dir\")\n","      flags.DEFINE_string(\"rst_dir\", active_learner1.folder_learn_iter[a_iter] + \"/test-rsts\", \"rst_dir\")\n","      flags.DEFINE_string(\"gt_dir\", active_learner1.folder_test_set + '/labels', \"gt_dir\")\n","      flags.DEFINE_string(\"rst_file\", active_learner1.folder_learn_iter[a_iter] + \"_test_rst.txt\", \"gt_dir\")\n","      flags.DEFINE_string(\"gpu\", '0', \"gpu\")\n","      FLAGS = flags.FLAGS\n","      \n","      os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu\n","      \n","      test_all=True\n","      color_table = load_color_table(path_workspace + '/labels.json')\n","      run_config = tf.ConfigProto()\n","      sess=tf.Session(config=run_config)\n","      with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n","          net = DeepLab(\n","                sess,\n","                input_width=img_size[0],\n","                input_height=img_size[1],\n","                batch_size=1,\n","                img_pattern=\"*.jpg\",\n","                label_pattern=\"*.png\",\n","                checkpoint_dir=FLAGS.checkpoint_dir,\n","                pretrain_dir='',\n","                train_dataset=FLAGS.train_dataset,\n","                val_dataset=FLAGS.val_dataset,\n","                num_class=2,\n","                color_table=color_table,is_train=False)\n","          if not net.load(net.checkpoint_dir)[0]:\n","              raise Exception(\"Cannot find checkpoint!\")\n","              \n","          if test_all:\n","          \n","              #test on train\n","              img_dir = FLAGS.img_dir\n","              rst_dir = FLAGS.rst_dir\n","              gt_dir = FLAGS.gt_dir\n","              \n","              if not os.path.exists(rst_dir):\n","                  os.makedirs(rst_dir)\n","      \n","              \n","              files=os.listdir(img_dir)\n","              for i,file in enumerate(files):\n","                  if not file.endswith(\".jpg\"):\n","                      continue\n","                  \n","                  \n","                  img = cv2.imread(os.path.join(img_dir,file))\n","                  # print(img_dir)\n","                  # print(file)\n","                  \n","                  idxmap, colormap, out_put_pd = net.inference(img)  \n","                  \n","                  # active_learner1.add_instance(img_pd = out_put_pd, file_name = file)\n","                  # al.show_prob_distribution('image_porb', out_put_pd)\n","                  \n","                  colormap=cv2.cvtColor(colormap,cv2.COLOR_RGB2BGR)\n","                  \n","                  cv2.imwrite(os.path.join(rst_dir,file[:-4]+'.png'),colormap) \n","                  \n","              mean_dice, mean_iou = evaluate_seg_result(rst_dir, gt_dir, FLAGS.rst_file)\n","\n","              m_iou_nb = evaluation_contour.evaluate_IOU_nearContour(output_path = active_learner1.folder_learn_iter[a_iter] + \"/test-rsts\",\n","                                                                     image_path = active_learner1.folder_test_set + '/images',\n","                                                                     label_path = active_learner1.folder_test_set + '/labels')\n","              test_dice.append(mean_dice)\n","              test_iou.append(mean_iou)\n","              test_iou_nb.append(m_iou_nb)\n","\n","              def del_all_flags(FLAGS):\n","                  flags_dict = FLAGS._flags()\n","                  keys_list = [keys for keys in flags_dict]\n","                  for keys in keys_list:\n","                      FLAGS.__delattr__(keys)\n","              del_all_flags(tf.flags.FLAGS)\n","              # tf.reset_default_graph()\n","\n","\n","              flags = tf.app.flags\n","              tf.app.flags.DEFINE_string('f', '', 'kernel')\n","              flags.DEFINE_string(\"train_dataset\", active_learner1.folder_labeled[a_iter], \"train dataset direction\")\n","              flags.DEFINE_string(\"val_dataset\", active_learner1.folder_unlabeled[a_iter], \"val dataset direction\")\n","              flags.DEFINE_string(\"checkpoint_dir\", active_learner1.folder_checkpoint[a_iter] + check_point_folder_name, \"checkpoint\")\n","              flags.DEFINE_string(\"img_dir\", active_learner1.folder_test_set + '/images', \"img_dir\")\n","              flags.DEFINE_string(\"rst_dir\", active_learner1.folder_learn_iter[a_iter] + \"/test-rsts\", \"rst_dir\")\n","              flags.DEFINE_string(\"gt_dir\", active_learner1.folder_test_set + '/labels', \"gt_dir\")\n","              flags.DEFINE_string(\"rst_file\", active_learner1.folder_learn_iter[a_iter] + \"_test_rst.txt\", \"gt_dir\")\n","              flags.DEFINE_string(\"gpu\", '0', \"gpu\")\n","              FLAGS = flags.FLAGS\n","\n","\n","              os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu\n","              test_all=True\n","              color_table = load_color_table(path_workspace + '/labels.json')\n","              run_config = tf.ConfigProto()\n","              sess=tf.Session(config=run_config)\n","              with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n","                  net = DeepLab(\n","                        sess,\n","                        input_width=img_size[0],\n","                        input_height=img_size[1],\n","                        batch_size=1,\n","                        img_pattern=\"*.jpg\",\n","                        label_pattern=\"*.png\",\n","                        checkpoint_dir=FLAGS.checkpoint_dir,\n","                        pretrain_dir='',\n","                        train_dataset=FLAGS.train_dataset,\n","                        val_dataset=FLAGS.val_dataset,\n","                        num_class=2,\n","                        color_table=color_table,is_train=False)\n","                  if not net.load(net.checkpoint_dir)[0]:\n","                      raise Exception(\"Cannot find checkpoint!\")\n","                      \n","                  \n","                  \n","                  if test_all:\n","                  \n","                      #test on train\n","                      img_dir = FLAGS.img_dir\n","                      rst_dir = FLAGS.rst_dir\n","                      gt_dir = FLAGS.gt_dir\n","                      if not os.path.exists(rst_dir):\n","                          os.makedirs(rst_dir)\n","\n","                      \n","                      files=os.listdir(img_dir)\n","                      stds_iou = []\n","                      means_iou = []\n","                      for i,file in enumerate(files):\n","                          if not file.endswith(\".jpg\"):\n","                              continue\n","                          \n","                          ious = []\n","                          for i in range(6):\n","                              img = Image.open(os.path.join(img_dir,file))\n","                              img = img.rotate(i*60)\n","                              img = np.asarray(img)\n","                              img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n","                              \n","                              gt = Image.open(os.path.join(gt_dir,file[:-4]+'.png'))\n","                              gt = gt.rotate(i*60)\n","                              gt = np.asarray(gt)\n","                                      \n","                          \n","                              idxmap, colormap, out_put_pd = net.inference(img)  \n","                          \n","                              gt_=1-gt\n","                              #     \n","                              output = cv2.resize(idxmap,(gt.shape[1],gt.shape[0]),interpolation=cv2.INTER_NEAREST)\n","                              output_=1-output\n","                          \n","                              #\n","                              if (np.count_nonzero(output)+np.count_nonzero(gt)) is 0:\n","                                  iou = 1\n","                              else:                        \n","                                  iou = np.count_nonzero(gt*output)/(np.count_nonzero(output+gt)+0.000001)\n","                              \n","                              ious.append(iou)\n","                          \n","                          m_iou = np.mean(ious)\n","                          std_iou = np.std(ious)\n","                          stds_iou.append(std_iou)\n","                          means_iou.append(m_iou)\n","                      # print(\"mean_iou_mean: {},  mean_iou_var: {}\".format(np.mean(means_iou),np.mean(stds_iou)))\n","                      m_iou_rs = np.mean(means_iou)\n","                      m_iou_rs_std = np.mean(stds_iou)\n","              \n","              \n","              test_iou_rs.append([m_iou_rs, m_iou_rs_std])\n","\n","\n","\n","      print('[Active Learner]: Active learning interation complete, completed iteration:' + str(a_iter))\n","      \n","  print('Active training ended, validation result:')\n","  print(vali_evaluation_result)\n","  print('Active training ended, test results:')\n","  print('DICE:')\n","  print(test_dice)\n","  print('IOU:')\n","  print(test_iou)\n","  print('IOU_nb:')\n","  print(test_iou_nb)\n","  print('IOU_rs:')\n","  print(test_iou_rs)\n","\n","  print('Total time cost(second):')\n","  print(str(time.time()-start_time))\n","\n","  final_vali_rst.append(vali_evaluation_result)\n","  final_test_dice.append(test_dice)\n","  final_test_iou.append(test_iou)\n","  final_test_iou_nb.append(test_iou_nb)\n","  final_test_iou_rs.append(test_iou_rs)\n","  \n","\n","# print('Active training ended,fianl validation results of all attemptions:')\n","# print(final_vali_rst)\n","print('Active training ended, final test results of all attemptions:')\n","print('DICE of each trail:')\n","print(final_test_dice)\n","print('Final IOU of each trail:')\n","print(final_test_iou)\n","print('Final IOU_nb of each trail:')\n","print(final_test_iou_nb)\n","# print('Final IOU_rs:')\n","# print(final_test_iou_rs)\n","\n","print('Result of the final result:')\n","print('Dice:')\n","print(np.array(final_test_dice)[:,-1])\n","print('IOU:')\n","print(np.array(final_test_iou)[:,-1])\n","print('IOU NB:')\n","print(np.array(final_test_iou_nb)[:,-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":50077163,"status":"error","timestamp":1621325084710,"user":{"displayName":"Haonan Peng","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4GmArzFW7K2Q2E1X7f2__FMNgtwjOtMJYWb1k=s64","userId":"08169864112056329318"},"user_tz":420},"id":"mw_bhWZm2plv","outputId":"9103bc2a-8569-49c7-bb6f-b63c837e61d0"},"outputs":[],"source":["sys.exit()\n","print('active training complete, begin to copy files')\n","%cp -ar '/content/active_learning_v1' '/content/drive/MyDrive/active_learning_ws/active_learning_v1/test_attempts'\n","print('Copy files complete')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNcnOdKZoZRZTjnCgvWYSWT","collapsed_sections":[],"mount_file_id":"1aTYhxAxHLtkY4ZAS9ogCrd-IvToyIAKJ","name":"multi_attempt_active_train_colab_CADAVER_BALD_2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
